\section{How to setup a simulation?}

When you perform a simulation, you are making a series of
approximations.  The first comes with the choice of equation set.  As
discussed earlier, for pure hydrodynamics, this often means solving
the Euler rather than the full Navier-Stokes equations.  The next
approximation is made when discretizing the equations, using the 
methods we described in the earlier chapters.  

Astrophysical flows involve a range of length and timescales.  Often
it is not possible to capture all these scales in a single simulation,
so some comprimises must be made.  If you are interested in the
dynamics on the full scale of the system, $L$, then you will resolve
down to some cutoff scale $l$, where $L/l \sim O(10^3--10^4)$.  At scales
smaller than $l$, you neglect the flow features.  The smallest
scale of importance is the disipation scale $\lambda \ll L$.  

\section{Picking your resolution}

To capture turbulence, you need a large range of lengthscales to get a
high Reynolds number.  Generally, you'll just start to get a resolved
inertial range over a decade in wavenumber in your power spectrum at
around 512 zones on a side of your domain.  With fewer points than
this, you are unlikely to see a turbulent cascade.

If nuclear burning is modeled, the steepness of the reaction rate will
set a lengthscale.

Hydrostatic equilibrium also puts demands on the resolution.  A good
rule of thumb is that you need 10 points per pressure scale height to
get a reasonable hydrostatic balance.

At the end of the day, the best thing you can do is 
Convergence study




\section{Boundary conditions}

Another approximation you make is how to treat the boundaries.
Generally speaking, you should put your boundaries as far away from
the place where the action is occuring as possible.  For example,
if you are modeling a convective region bounded by stably stratified
flow, then you should have a large buffer of stable atmosphere
bounding it.

Periodic boundary conditions don't magically fix everything either.  
With a triply-periodic box (periodic on all boundaries), then you 
are confining the flow.  If energy is injected into the box, then
there is no place for it to expand.  


wall heating


traditional


characteristic BCs?





\section{Timestep considerations}

Burning timescale



\section{Computational cost}

The main limitation to your choice of resolution and timestep is
computational cost.  For a three-dimensional simulation in a box with
$N$ zones on a side, the work scales as $O(N^4)$.  The cost to advance
a single timestep is based on the volume, $N^3$, and the number of
timesteps to get you to a fixed simulation time, $t_\mathrm{max}$, is
$t_\mathrm{max}/\Delta t$, but $\Delta t \sim \Delta x / s \sim L/(N
s)$, where $s$ is the fastest information speed.  So the number 
of timesteps increases with $N$, making the total cost $N^4$.
Think about this for a minute---if you double the number of zones, then
the amount of work you need to do increases by 16$\times$.

A common conflict that arises is: Do you do one ``hero'' calculation
or many smaller calculations to understand the sensitivity of your
results to input parameters?  Computing centers that grant allocations
usually setup their queues to favor big jobs that use as much of the
machine as possible (as that's what their funders use as a metric for
their success).


It is important to understand that no single algorithm will offer everything
you need.


\section{I/O}

Another challenge with simulations is data storage.  For a calculation
on a $1024^3$ grid, using double precision, each number you store
takes 8~bytes / zone, so a single variable on the grid will require
8~GB.  For compressible hydro, you will have atleast 5 variables (in
3-d), $\rho$, $(\rho {\bf U})$, and $(\rho E)$, so this is a minimum of 40~GB per
file.  Often you will want some derived quantities (like temperature)
and composition quantities in your output, which will greatly increase
your storage requirements.

For this reason, simulation codes often have different types of
output.  Checkpoint files store all the variables that are needed to
restart the calculation from where you left off---these should be
stored in a (portable) binary format at machine precision.  You don't
need to keep these around forever, perhaps only saving the last few as
necessary to restart your calculation as it works through a computing
center's queue.  Plotfiles store a lot of variables---basically anything
that will be needed for the analysis of the simulation.  Since you don't
need these to restart, you can probably afford to store the variables
at single precision.  Sometimes you might have multiple levels of plotfiles,
storing a few variables (like density) very frequently to allow for 
the production of smooth movies, and storing a lot of variables and 
derived quantities at a much longer cadence.

A perpetual challenge with the analysis and visualization of
simulations is that sometimes you don't know what you are looking for
until the simulation is complete, so you cannot simply do runtime
visualization and not save the raw data as the simulation progresses.
This leads to collections of plotfiles that can easily top 100~TB per
simulation.  Runtime diagnostics can help a little---if you know any
global quantities that you will be interested in over the course of
the simulation (like peak temperature, or total energy) you can
compute these on the fly and simply store a single number per step (or
every $N$ steps) into a file.
